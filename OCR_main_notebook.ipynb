{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "OCR_main_notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNvtBlfibDrBbotd7sg4Fl2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hejazn86/OCR_Persian_textDetecter/blob/master/OCR_main_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vC2SZtcWSeiu",
        "outputId": "2e2045c3-3af2-49ee-cf68-db2a5df36e6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/drive')\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /drive; to attempt to forcibly remount, call drive.mount(\"/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czp97fehIrpG"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqtD5MiFRYiT"
      },
      "source": [
        "words_path = '/drive/My Drive/datasets/words/words.csv'\n",
        "labels = []\n",
        "with open(words_path) as csvfile:\n",
        "  reader = csv.reader(csvfile)\n",
        "  next(reader)\n",
        "  for i, row in enumerate(reader):\n",
        "    labels.append(row[0])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1e6yQ9rgRe2p",
        "outputId": "e768e85e-c69b-4e7d-ff06-7c779a1a94bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "label1 = []\n",
        "label2 = []\n",
        "label3 = []\n",
        "label4 = []\n",
        "inced = []\n",
        "for i, leb in enumerate(labels):\n",
        "  if len(leb)==8:\n",
        "    label1.append(leb)\n",
        "  elif len(leb)==6:\n",
        "    label2.append(leb)\n",
        "  elif len(leb)==7:\n",
        "    label3.append(leb)\n",
        "  elif len(leb)==5:\n",
        "    label4.append(leb)\n",
        "    inced.append(i)\n",
        "    \n",
        "\n",
        "print(len(label1))\n",
        "print(len(label2))\n",
        "print(len(label3)) \n",
        "print(len(label4))\n",
        "print(len(inced)) \n",
        "print(inced[1])\n",
        "\n",
        "characters = set(char for label in label4 for char in label)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3494\n",
            "2396\n",
            "2407\n",
            "1158\n",
            "1158\n",
            "244\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyxQlKfBRoGY",
        "outputId": "480edcdb-ef99-400a-9bbf-e1118402fe66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "path = '/drive/My Drive/datasets/images_with_persian_text'\n",
        "\n",
        "\n",
        "# Get lists of all the images, the labels, and the characters\n",
        "def loadImage(path):\n",
        "  image_files = sorted([os.path.join(path, file) for file in os.listdir(path) if file.endswith(\".png\")])\n",
        "  return image_files\n",
        "\n",
        "images1 = loadImage(path)\n",
        "\n",
        "images = []\n",
        "for l in inced:\n",
        "  for i,img in enumerate(images1):\n",
        "    if i==l:\n",
        "      images.append(img)\n",
        "\n",
        "\n",
        "#print(labels[0])\n",
        "#print(images[10])\n",
        "#img = cv2.imread(images[0], cv2.IMREAD_UNCHANGED)\n",
        "#plt.imshow(img)\n",
        "\n",
        "print(\"Number of images: \", len(images))\n",
        "print(\"Number of labels: \", len(label4))\n",
        "print(\"Number of unique: \", len(characters))\n",
        "print(\"Characters present: \", characters)\n",
        "\n",
        "# Batch size for training and validation\n",
        "batch_size = 20\n",
        "\n",
        "# Desired image dimensions\n",
        "img_width = 200\n",
        "img_height = 50\n",
        "\n",
        "'''Factor by which the image is going to be downsampled\n",
        " by the convolutional blocks. We will be using two\n",
        " convolution blocks and each block will have\n",
        " a pooling layer which downsample the features by a factor of 2.\n",
        " Hence total downsampling factor would be 4.'''\n",
        "\n",
        "downsample_factor = 4\n",
        "\n",
        "# Maximum length of any captcha in the dataset\n",
        "max_length = max([len(label) for label in label4])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of images:  1158\n",
            "Number of labels:  1158\n",
            "Number of unique:  35\n",
            "Characters present:  {'ن', 'د', 'م', 'ش', 'ع', 'س', 'ظ', 'ر', 'ء', 'ی', '\\u200c', 'ث', 'آ', 'ز', 'گ', 'خ', 'ق', 'ف', 'ه', 'ذ', 'ص', 'ک', 'و', 'ب', 'غ', 'ل', 'پ', 'ت', 'ض', 'چ', 'ح', 'ج', 'ا', 'ط', 'ژ'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "by6km0fNR6Nm"
      },
      "source": [
        "# Mapping characters to integers\n",
        "char_to_num = layers.experimental.preprocessing.StringLookup(\n",
        "    vocabulary=list(characters), num_oov_indices=0, mask_token=None\n",
        ")\n",
        "\n",
        "# Mapping integers back to original characters\n",
        "num_to_char = layers.experimental.preprocessing.StringLookup(\n",
        "    vocabulary=char_to_num.get_vocabulary(), mask_token=None, invert=True\n",
        ")\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oZ-E2ZqR0eJ",
        "outputId": "e684a084-5ed0-4de3-f57b-51d1cf37f19f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        }
      },
      "source": [
        "\n",
        "def split_data(images, labels, train_size=0.9, shuffle=True):\n",
        "    # 1. Get the total size of the dataset\n",
        "    size = len(images)\n",
        "    # 2. Make an indices array and shuffle it, if required\n",
        "    indices = np.arange(size)\n",
        "    if shuffle:\n",
        "        np.random.shuffle(indices)\n",
        "    # 3. Get the size of training samples\n",
        "    train_samples = int(size * train_size)\n",
        "    # 4. Split data into training and validation sets\n",
        "    x_train, y_train = images[indices[:train_samples]], label4[indices[:train_samples]]\n",
        "    x_valid, y_valid = images[indices[train_samples:]], label4[indices[train_samples:]]\n",
        "    return x_train, x_valid, y_train, y_valid\n",
        "\n",
        "\n",
        "# Splitting data into training and validation sets\n",
        "x_train, x_valid, y_train, y_valid = split_data(np.array(images), np.array(label4))\n",
        "\n",
        "\n",
        "def encode_single_sample(img_path, label):\n",
        "    # 1. Read image\n",
        "    img = tf.io.read_file(img_path)\n",
        "    # 2. Decode and convert to grayscale\n",
        "    img = tf.io.decode_png(img, channels=1)\n",
        "    # 3. Convert to float32 in [0, 1] range\n",
        "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "    # 4. Resize to the desired size\n",
        "    img = tf.image.resize(img, [img_height, img_width])\n",
        "    # 5. Transpose the image because we want the time\n",
        "    # dimension correspond to the width of the image.\n",
        "    img = tf.transpose(img, perm=[1, 0, 2])\n",
        "    # 6. Map the characters in label to numbers\n",
        "    label = char_to_num(tf.strings.unicode_split(label, input_encoding=\"UTF-8\"))\n",
        "    # 7. Return a dict as our model is expecting two inputs\n",
        "    return {\"image\": img, \"label\": label}\n",
        "\n",
        "print(len(x_train))\n",
        "print(len(x_valid))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-1dfd18275692>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Splitting data into training and validation sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-1dfd18275692>\u001b[0m in \u001b[0;36msplit_data\u001b[0;34m(images, labels, train_size, shuffle)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtrain_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# 4. Split data into training and validation sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrain_samples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel4\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrain_samples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mx_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel4\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKIw5g16eSyn",
        "outputId": "30da30f6-f67e-424d-84b2-bdd79fa9f14d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# creating the datasets\n",
        "def create_dataset(image_data, label_data):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((image_data, label_data))\n",
        "  dataset = (dataset.map(encode_single_sample, num_parallel_calls=1000))\n",
        "  dataset = dataset.batch(batch_size)\n",
        "  dataset = dataset.prefetch(buffer_size=1000)\n",
        "  return dataset\n",
        "\n",
        "\n",
        "train_dataset = create_dataset(x_train, y_train)\n",
        "validation_dataset = create_dataset(x_valid, y_valid)\n",
        "\n",
        "print(len(train_dataset))\n",
        "print(len(validation_dataset))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1050\n",
            "450\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1Y1LDs5hqUj"
      },
      "source": [
        "'''figure = figsize=(10, 5)\n",
        "for batch in train_dataset.take(1):\n",
        "    images = batch[\"image\"]\n",
        "    labels = batch[\"label\"]\n",
        "    img = (images[0] * 255).numpy().astype(\"uint8\")\n",
        "    label = tf.strings.reduce_join(num_to_char(labels[0])).numpy().decode(\"utf-8\")\n",
        "    ax[0].imshow(img[:, :, 0].T, cmap=\"gray\")\n",
        "    ax[0].set_title(label)\n",
        "    ax[0].axis(\"off\")\n",
        "plt.show()'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3vasf8ElJ1l",
        "outputId": "d29551a5-8ec8-4b02-a385-94fcdca47388",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 624
        }
      },
      "source": [
        "# The model \n",
        "class CTCLayer(layers.Layer):\n",
        "    def __init__(self, name=None):\n",
        "        super().__init__(name=name)\n",
        "        self.loss_fn = keras.backend.ctc_batch_cost\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n",
        "        input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n",
        "        label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n",
        "\n",
        "        input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
        "        label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
        "\n",
        "        loss = self.loss_fn(y_true, y_pred, input_length, label_length)\n",
        "        self.add_loss(loss)\n",
        "        return y_pred\n",
        "\n",
        "\n",
        "def build_model():\n",
        "    input_img = layers.Input(shape=(img_width, img_height, 1), name=\"image\", dtype=\"float32\")\n",
        "    labels = layers.Input(name=\"label\", shape=(None,), dtype=\"float32\")\n",
        "    # CNNs\n",
        "    x = layers.Conv2D(32, (3, 3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\", name=\"Conv1\",)(input_img)\n",
        "    x = layers.MaxPooling2D((2, 2), name=\"pool1\")(x)\n",
        "\n",
        "    x = layers.Conv2D(64,  (3, 3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\", name=\"Conv2\",)(x)\n",
        "    x = layers.MaxPooling2D((2, 2), name=\"pool2\")(x)\n",
        "\n",
        "    ''' We have used two max pool with pool size and strides 2.\n",
        "     Hence, downsampled feature maps are 4x smaller. The number of\n",
        "     filters in the last layer is 64. Reshape accordingly before\n",
        "     passing the output to the RNN part of the model'''\n",
        "    new_shape = ((img_width // 4), (img_height // 4) * 64)\n",
        "    x = layers.Reshape(target_shape=new_shape, name=\"reshape\")(x)\n",
        "    x = layers.Dense(64, activation=\"relu\", name=\"dense1\")(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "\n",
        "    # RNNs\n",
        "    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True, dropout=0.25))(x)\n",
        "    x = layers.Bidirectional(layers.LSTM(64, return_sequences=True, dropout=0.25))(x)\n",
        "\n",
        "    # Output layer\n",
        "    x = layers.Dense(len(characters) + 1, activation=\"softmax\", name=\"dense2\")(x)\n",
        "\n",
        "    # Add CTC layer for calculating CTC loss at each step\n",
        "    output = CTCLayer(name=\"ctc_loss\")(labels, x)\n",
        "\n",
        "    # Define the model\n",
        "    model = keras.models.Model(inputs=[input_img, labels], outputs=output, name=\"ocr_model_v1\")\n",
        "\n",
        "    # Optimizer\n",
        "    opt = keras.optimizers.Adam()\n",
        "    # Compile the model and return\n",
        "    model.compile(optimizer=opt)\n",
        "    return model\n",
        "\n",
        "\n",
        "model = build_model()\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"ocr_model_v1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "image (InputLayer)              [(None, 200, 50, 1)] 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Conv1 (Conv2D)                  (None, 200, 50, 32)  320         image[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "pool1 (MaxPooling2D)            (None, 100, 25, 32)  0           Conv1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "Conv2 (Conv2D)                  (None, 100, 25, 64)  18496       pool1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "pool2 (MaxPooling2D)            (None, 50, 12, 64)   0           Conv2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "reshape (Reshape)               (None, 50, 768)      0           pool2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense1 (Dense)                  (None, 50, 64)       49216       reshape[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 50, 64)       0           dense1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional (Bidirectional)   (None, 50, 256)      197632      dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_1 (Bidirectional) (None, 50, 128)      164352      bidirectional[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "label (InputLayer)              [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense2 (Dense)                  (None, 50, 37)       4773        bidirectional_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "ctc_loss (CTCLayer)             (None, 50, 37)       0           label[0][0]                      \n",
            "                                                                 dense2[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 434,789\n",
            "Trainable params: 434,789\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YB01bT7DlTl4",
        "outputId": "79ba455e-e528-4a86-d364-2c33a227ad15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        }
      },
      "source": [
        "# Training the model\n",
        "epochs = 100\n",
        "early_stopping_patience = 10\n",
        "\n",
        "# Add early stopping\n",
        "early_stopping = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=early_stopping_patience, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(train_dataset, validation_data=validation_dataset, epochs=epochs, callbacks=[early_stopping])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-f292a7f6d191>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 840\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m:  Cannot add tensor to the batch: number of elements does not match. Shapes are: [tensor]: [13], [batch]: [11]\n\t [[node IteratorGetNext (defined at <ipython-input-14-f292a7f6d191>:14) ]] [Op:__inference_train_function_12353]\n\nFunction call stack:\ntrain_function\n"
          ]
        }
      ]
    }
  ]
}