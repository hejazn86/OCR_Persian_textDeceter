{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "OCR_main_notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO33A7IzOpgu++Oo4sOISlc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hejazn86/OCR_Persian_textDetecter/blob/master/OCR_main_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vC2SZtcWSeiu",
        "outputId": "fa44d5dc-f1f7-4e57-dbbb-b8054c03d4bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/drive')\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czp97fehIrpG"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqtD5MiFRYiT"
      },
      "source": [
        "words_path = '/drive/My Drive/datasets/words/words.csv'\n",
        "labels = []\n",
        "with open(words_path) as csvfile:\n",
        "  reader = csv.reader(csvfile)\n",
        "  next(reader)\n",
        "  for i, row in enumerate(reader):\n",
        "    labels.append(row[0])"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1e6yQ9rgRe2p",
        "outputId": "619f6068-abb6-477f-a8f2-fc2aab377975",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "label1 = []\n",
        "label2 = []\n",
        "label3 = []\n",
        "label4 = []\n",
        "inced = []\n",
        "for i, leb in enumerate(labels):\n",
        "  if len(leb)==8:\n",
        "    label1.append(leb)\n",
        "  elif len(leb)==6:\n",
        "    label2.append(leb)\n",
        "  elif len(leb)==7:\n",
        "    label3.append(leb)\n",
        "  elif len(leb)==5:\n",
        "    label4.append(leb)\n",
        "    inced.append(i)\n",
        "    \n",
        "\n",
        "print(len(label1))\n",
        "print(len(label2))\n",
        "print(len(label3)) \n",
        "print(len(label4))\n",
        "print(len(inced)) \n",
        "print(inced[1])\n",
        "\n",
        "characters = set(char for label in label4 for char in label)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3494\n",
            "2396\n",
            "2407\n",
            "1158\n",
            "1158\n",
            "244\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyxQlKfBRoGY",
        "outputId": "73a90953-1653-4088-b1ed-9b2e8926f492",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "path = '/drive/My Drive/datasets/images_with_persian_text'\n",
        "\n",
        "\n",
        "# Get lists of all the images, the labels, and the characters\n",
        "def loadImage(path):\n",
        "  image_files = sorted([os.path.join(path, file) for file in os.listdir(path) if file.endswith(\".png\")])\n",
        "  return image_files\n",
        "\n",
        "images1 = loadImage(path)\n",
        "\n",
        "images = []\n",
        "for l in inced:\n",
        "  for i,img in enumerate(images1):\n",
        "    if i==l:\n",
        "      images.append(img)\n",
        "\n",
        "\n",
        "#print(labels[0])\n",
        "#print(images[10])\n",
        "#img = cv2.imread(images[0], cv2.IMREAD_UNCHANGED)\n",
        "#plt.imshow(img)\n",
        "\n",
        "print(\"Number of images: \", len(images))\n",
        "print(\"Number of labels: \", len(label4))\n",
        "print(\"Number of unique: \", len(characters))\n",
        "print(\"Characters present: \", characters)\n",
        "\n",
        "# Batch size for training and validation\n",
        "batch_size = 20\n",
        "\n",
        "# Desired image dimensions\n",
        "img_width = 200\n",
        "img_height = 50\n",
        "\n",
        "'''Factor by which the image is going to be downsampled\n",
        " by the convolutional blocks. We will be using two\n",
        " convolution blocks and each block will have\n",
        " a pooling layer which downsample the features by a factor of 2.\n",
        " Hence total downsampling factor would be 4.'''\n",
        "\n",
        "downsample_factor = 4\n",
        "\n",
        "# Maximum length of any captcha in the dataset\n",
        "max_length = max([len(label) for label in label4])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of images:  1158\n",
            "Number of labels:  1158\n",
            "Number of unique:  35\n",
            "Characters present:  {'ت', 'ن', 'ظ', 'س', 'ر', 'ء', 'ع', 'ل', 'غ', 'ض', 'ط', 'ق', 'آ', 'ب', '\\u200c', 'ث', 'ص', 'ز', 'ی', 'ح', 'ژ', 'خ', 'ج', 'م', 'ش', 'د', 'ف', 'گ', 'پ', 'ذ', 'چ', 'و', 'ک', 'ا', 'ه'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "by6km0fNR6Nm"
      },
      "source": [
        "# Mapping characters to integers\n",
        "char_to_num = layers.experimental.preprocessing.StringLookup(\n",
        "    vocabulary=list(characters), num_oov_indices=0, mask_token=None\n",
        ")\n",
        "\n",
        "# Mapping integers back to original characters\n",
        "num_to_char = layers.experimental.preprocessing.StringLookup(\n",
        "    vocabulary=char_to_num.get_vocabulary(), mask_token=None, invert=True\n",
        ")\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oZ-E2ZqR0eJ",
        "outputId": "f4937509-d80e-4f77-d10b-2c93af73ac86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "def split_data(images, labels, train_size=0.9, shuffle=True):\n",
        "    # 1. Get the total size of the dataset\n",
        "    size = len(images)\n",
        "    # 2. Make an indices array and shuffle it, if required\n",
        "    if shuffle:\n",
        "        np.random.shuffle(indices)\n",
        "    # 3. Get the size of training samples\n",
        "    train_samples = int(size * train_size)\n",
        "    # 4. Split data into training and validation sets\n",
        "    x_train, y_train = images[:train_samples], label4[:train_samples]\n",
        "    x_valid, y_valid = images[train_samples:], label4[train_samples:]\n",
        "    return x_train, x_valid, y_train, y_valid\n",
        "\n",
        "\n",
        "# Splitting data into training and validation sets\n",
        "x_train, x_valid, y_train, y_valid = split_data(np.array(images), np.array(label4))\n",
        "\n",
        "\n",
        "def encode_single_sample(img_path, label):\n",
        "    # 1. Read image\n",
        "    img = tf.io.read_file(img_path)\n",
        "    # 2. Decode and convert to grayscale\n",
        "    img = tf.io.decode_png(img, channels=1)\n",
        "    # 3. Convert to float32 in [0, 1] range\n",
        "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "    # 4. Resize to the desired size\n",
        "    img = tf.image.resize(img, [img_height, img_width])\n",
        "    # 5. Transpose the image because we want the time\n",
        "    # dimension correspond to the width of the image.\n",
        "    img = tf.transpose(img, perm=[1, 0, 2])\n",
        "    # 6. Map the characters in label to numbers\n",
        "    label = char_to_num(tf.strings.unicode_split(label, input_encoding=\"UTF-8\"))\n",
        "    # 7. Return a dict as our model is expecting two inputs\n",
        "    return {\"image\": img, \"label\": label}\n",
        "\n",
        "print(len(x_train))\n",
        "print(len(x_valid))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1042\n",
            "116\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKIw5g16eSyn",
        "outputId": "49289f96-a86e-4727-8e38-824f321692ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# creating the datasets\n",
        "def create_dataset(image_data, label_data):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((image_data, label_data))\n",
        "  dataset = (dataset.map(encode_single_sample, num_parallel_calls=tf.data.experimental.AUTOTUNE))\n",
        "  dataset = dataset.batch(batch_size)\n",
        "  dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "  return dataset\n",
        "\n",
        "\n",
        "train_dataset = create_dataset(x_train, y_train)\n",
        "validation_dataset = create_dataset(x_valid, y_valid)\n",
        "\n",
        "print(len(train_dataset))\n",
        "print(len(validation_dataset))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "53\n",
            "6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1Y1LDs5hqUj",
        "outputId": "c578aa7f-5dd4-4df9-ffe8-6389aa42936b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "'''figure, ax = plt.subplots(figsize=(10, 5))\n",
        "for batch in train_dataset.take(1):\n",
        "    images = batch[\"image\"]\n",
        "    labels = batch[\"label\"]\n",
        "    img = (images[0] * 255).numpy().astype(\"uint8\")\n",
        "    label = tf.strings.reduce_join(num_to_char(labels[0])).numpy().decode(\"utf-8\")\n",
        "    ax[0].imshow(img[:, :, 0].T, cmap=\"gray\")\n",
        "    ax[0].set_title(label)\n",
        "    ax[0].axis(\"off\")\n",
        "plt.show()'''"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'figure, ax = plt.subplots(figsize=(10, 5))\\nfor batch in train_dataset.take(1):\\n    images = batch[\"image\"]\\n    labels = batch[\"label\"]\\n    img = (images[0] * 255).numpy().astype(\"uint8\")\\n    label = tf.strings.reduce_join(num_to_char(labels[0])).numpy().decode(\"utf-8\")\\n    ax[0].imshow(img[:, :, 0].T, cmap=\"gray\")\\n    ax[0].set_title(label)\\n    ax[0].axis(\"off\")\\nplt.show()'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3vasf8ElJ1l",
        "outputId": "1a328d62-6f16-4878-da3a-dcbbeb6ef731",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# The model \n",
        "class CTCLayer(layers.Layer):\n",
        "    def __init__(self, name=None):\n",
        "        super().__init__(name=name)\n",
        "        self.loss_fn = keras.backend.ctc_batch_cost\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n",
        "        input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n",
        "        label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n",
        "\n",
        "        input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
        "        label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
        "\n",
        "        loss = self.loss_fn(y_true, y_pred, input_length, label_length)\n",
        "        self.add_loss(loss)\n",
        "        return y_pred\n",
        "\n",
        "\n",
        "def build_model():\n",
        "    input_img = layers.Input(shape=(img_width, img_height, 1), name=\"image\", dtype=\"float32\")\n",
        "    labels = layers.Input(name=\"label\", shape=(None,), dtype=\"float32\")\n",
        "    # CNNs\n",
        "    x = layers.Conv2D(32, (3, 3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\", name=\"Conv1\",)(input_img)\n",
        "    x = layers.MaxPooling2D((2, 2), name=\"pool1\")(x)\n",
        "\n",
        "    x = layers.Conv2D(64,  (3, 3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\", name=\"Conv2\",)(x)\n",
        "    x = layers.MaxPooling2D((2, 2), name=\"pool2\")(x)\n",
        "\n",
        "    ''' We have used two max pool with pool size and strides 2.\n",
        "     Hence, downsampled feature maps are 4x smaller. The number of\n",
        "     filters in the last layer is 64. Reshape accordingly before\n",
        "     passing the output to the RNN part of the model'''\n",
        "    new_shape = ((img_width // 4), (img_height // 4) * 64)\n",
        "    x = layers.Reshape(target_shape=new_shape, name=\"reshape\")(x)\n",
        "    x = layers.Dense(64, activation=\"relu\", name=\"dense1\")(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "\n",
        "    # RNNs\n",
        "    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True, dropout=0.25))(x)\n",
        "    x = layers.Bidirectional(layers.LSTM(64, return_sequences=True, dropout=0.25))(x)\n",
        "\n",
        "    # Output layer\n",
        "    x = layers.Dense(len(characters) + 1, activation=\"softmax\", name=\"dense2\")(x)\n",
        "\n",
        "    # Add CTC layer for calculating CTC loss at each step\n",
        "    output = CTCLayer(name=\"ctc_loss\")(labels, x)\n",
        "\n",
        "    # Define the model\n",
        "    model = keras.models.Model(inputs=[input_img, labels], outputs=output, name=\"ocr_model_v1\")\n",
        "\n",
        "    # Optimizer\n",
        "    opt = keras.optimizers.Adam()\n",
        "    # Compile the model and return\n",
        "    model.compile(optimizer=opt)\n",
        "    return model\n",
        "\n",
        "\n",
        "model = build_model()\n",
        "model.summary()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"ocr_model_v1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "image (InputLayer)              [(None, 200, 50, 1)] 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Conv1 (Conv2D)                  (None, 200, 50, 32)  320         image[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "pool1 (MaxPooling2D)            (None, 100, 25, 32)  0           Conv1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "Conv2 (Conv2D)                  (None, 100, 25, 64)  18496       pool1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "pool2 (MaxPooling2D)            (None, 50, 12, 64)   0           Conv2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "reshape (Reshape)               (None, 50, 768)      0           pool2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense1 (Dense)                  (None, 50, 64)       49216       reshape[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 50, 64)       0           dense1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional (Bidirectional)   (None, 50, 256)      197632      dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_1 (Bidirectional) (None, 50, 128)      164352      bidirectional[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "label (InputLayer)              [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense2 (Dense)                  (None, 50, 36)       4644        bidirectional_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "ctc_loss (CTCLayer)             (None, 50, 36)       0           label[0][0]                      \n",
            "                                                                 dense2[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 434,660\n",
            "Trainable params: 434,660\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YB01bT7DlTl4",
        "outputId": "3e7d54d6-d207-40ed-c754-2caaaeec4390",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Training the model\n",
        "epochs = 100\n",
        "early_stopping_patience = 10\n",
        "\n",
        "# Add early stopping\n",
        "#early_stopping = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=early_stopping_patience, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(train_dataset, validation_data=validation_dataset, epochs=epochs)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "53/53 [==============================] - 23s 440ms/step - loss: 12.5219 - val_loss: 13.1580\n",
            "Epoch 2/100\n",
            "53/53 [==============================] - 24s 447ms/step - loss: 12.4518 - val_loss: 13.1265\n",
            "Epoch 3/100\n",
            "53/53 [==============================] - 23s 442ms/step - loss: 12.4656 - val_loss: 13.2606\n",
            "Epoch 4/100\n",
            "53/53 [==============================] - 23s 443ms/step - loss: 12.3874 - val_loss: 13.2887\n",
            "Epoch 5/100\n",
            "53/53 [==============================] - 24s 447ms/step - loss: 12.3582 - val_loss: 13.3355\n",
            "Epoch 6/100\n",
            "53/53 [==============================] - 23s 443ms/step - loss: 12.3406 - val_loss: 13.3796\n",
            "Epoch 7/100\n",
            "53/53 [==============================] - 24s 445ms/step - loss: 12.3206 - val_loss: 13.4316\n",
            "Epoch 8/100\n",
            "53/53 [==============================] - 24s 445ms/step - loss: 12.3104 - val_loss: 13.4676\n",
            "Epoch 9/100\n",
            "53/53 [==============================] - 24s 443ms/step - loss: 12.2939 - val_loss: 13.4989\n",
            "Epoch 10/100\n",
            "53/53 [==============================] - 24s 445ms/step - loss: 12.2943 - val_loss: 13.5249\n",
            "Epoch 11/100\n",
            "53/53 [==============================] - 29s 546ms/step - loss: 12.2809 - val_loss: 13.5489\n",
            "Epoch 12/100\n",
            "53/53 [==============================] - 24s 448ms/step - loss: 12.2830 - val_loss: 13.5554\n",
            "Epoch 13/100\n",
            "53/53 [==============================] - 24s 445ms/step - loss: 12.2643 - val_loss: 13.5533\n",
            "Epoch 14/100\n",
            "53/53 [==============================] - 24s 448ms/step - loss: 12.2510 - val_loss: 13.5732\n",
            "Epoch 15/100\n",
            "53/53 [==============================] - 24s 451ms/step - loss: 12.2507 - val_loss: 13.5770\n",
            "Epoch 16/100\n",
            "53/53 [==============================] - 24s 444ms/step - loss: 12.2392 - val_loss: 13.5880\n",
            "Epoch 17/100\n",
            "53/53 [==============================] - 23s 441ms/step - loss: 12.2395 - val_loss: 13.5965\n",
            "Epoch 18/100\n",
            "53/53 [==============================] - 24s 445ms/step - loss: 12.2335 - val_loss: 13.6004\n",
            "Epoch 19/100\n",
            "53/53 [==============================] - 24s 444ms/step - loss: 12.2306 - val_loss: 13.6133\n",
            "Epoch 20/100\n",
            "53/53 [==============================] - 24s 443ms/step - loss: 12.2256 - val_loss: 13.6216\n",
            "Epoch 21/100\n",
            "53/53 [==============================] - 24s 448ms/step - loss: 12.2200 - val_loss: 13.6319\n",
            "Epoch 22/100\n",
            "53/53 [==============================] - 24s 444ms/step - loss: 12.2176 - val_loss: 13.6459\n",
            "Epoch 23/100\n",
            "53/53 [==============================] - 23s 441ms/step - loss: 12.2168 - val_loss: 13.6485\n",
            "Epoch 24/100\n",
            "53/53 [==============================] - 24s 450ms/step - loss: 12.2107 - val_loss: 13.6504\n",
            "Epoch 25/100\n",
            "53/53 [==============================] - 24s 451ms/step - loss: 12.2072 - val_loss: 13.6617\n",
            "Epoch 26/100\n",
            "53/53 [==============================] - 24s 444ms/step - loss: 12.2039 - val_loss: 13.6658\n",
            "Epoch 27/100\n",
            "53/53 [==============================] - 24s 446ms/step - loss: 12.2034 - val_loss: 13.6743\n",
            "Epoch 28/100\n",
            "53/53 [==============================] - 24s 445ms/step - loss: 12.1988 - val_loss: 13.6820\n",
            "Epoch 29/100\n",
            "53/53 [==============================] - 23s 442ms/step - loss: 12.1962 - val_loss: 13.6854\n",
            "Epoch 30/100\n",
            "53/53 [==============================] - 24s 446ms/step - loss: 12.1873 - val_loss: 13.6945\n",
            "Epoch 31/100\n",
            "53/53 [==============================] - 23s 443ms/step - loss: 12.1949 - val_loss: 13.7006\n",
            "Epoch 32/100\n",
            "53/53 [==============================] - 23s 442ms/step - loss: 12.1851 - val_loss: 13.7054\n",
            "Epoch 33/100\n",
            "53/53 [==============================] - 23s 442ms/step - loss: 12.1807 - val_loss: 13.7126\n",
            "Epoch 34/100\n",
            "53/53 [==============================] - 23s 442ms/step - loss: 12.1785 - val_loss: 13.7131\n",
            "Epoch 35/100\n",
            "53/53 [==============================] - 24s 445ms/step - loss: 12.1771 - val_loss: 13.7168\n",
            "Epoch 36/100\n",
            "53/53 [==============================] - 24s 447ms/step - loss: 12.1764 - val_loss: 13.7134\n",
            "Epoch 37/100\n",
            "53/53 [==============================] - 24s 447ms/step - loss: 12.1763 - val_loss: 13.7131\n",
            "Epoch 38/100\n",
            "53/53 [==============================] - 24s 450ms/step - loss: 12.1730 - val_loss: 13.7176\n",
            "Epoch 39/100\n",
            "53/53 [==============================] - 24s 446ms/step - loss: 12.1699 - val_loss: 13.7205\n",
            "Epoch 40/100\n",
            "53/53 [==============================] - 24s 450ms/step - loss: 12.1703 - val_loss: 13.7253\n",
            "Epoch 41/100\n",
            "53/53 [==============================] - 24s 446ms/step - loss: 12.1702 - val_loss: 13.7243\n",
            "Epoch 42/100\n",
            "53/53 [==============================] - 24s 446ms/step - loss: 12.1659 - val_loss: 13.7312\n",
            "Epoch 43/100\n",
            "53/53 [==============================] - 24s 450ms/step - loss: 12.1689 - val_loss: 13.7358\n",
            "Epoch 44/100\n",
            "53/53 [==============================] - 24s 447ms/step - loss: 12.1651 - val_loss: 13.7379\n",
            "Epoch 45/100\n",
            "53/53 [==============================] - 24s 449ms/step - loss: 12.1668 - val_loss: 13.7395\n",
            "Epoch 46/100\n",
            "53/53 [==============================] - 24s 445ms/step - loss: 12.1619 - val_loss: 13.7409\n",
            "Epoch 47/100\n",
            "53/53 [==============================] - 24s 447ms/step - loss: 12.1625 - val_loss: 13.7399\n",
            "Epoch 48/100\n",
            "53/53 [==============================] - 24s 443ms/step - loss: 12.1588 - val_loss: 13.7465\n",
            "Epoch 49/100\n",
            "53/53 [==============================] - 24s 445ms/step - loss: 12.1572 - val_loss: 13.7511\n",
            "Epoch 50/100\n",
            "53/53 [==============================] - 24s 448ms/step - loss: 12.1539 - val_loss: 13.7555\n",
            "Epoch 51/100\n",
            "53/53 [==============================] - 28s 538ms/step - loss: 12.1560 - val_loss: 13.7546\n",
            "Epoch 52/100\n",
            "53/53 [==============================] - 24s 445ms/step - loss: 12.1523 - val_loss: 13.7576\n",
            "Epoch 53/100\n",
            "53/53 [==============================] - 24s 445ms/step - loss: 12.1510 - val_loss: 13.7585\n",
            "Epoch 54/100\n",
            "53/53 [==============================] - 23s 440ms/step - loss: 12.1507 - val_loss: 13.7595\n",
            "Epoch 55/100\n",
            "53/53 [==============================] - 24s 446ms/step - loss: 12.1471 - val_loss: 13.7642\n",
            "Epoch 56/100\n",
            "53/53 [==============================] - 24s 444ms/step - loss: 12.1468 - val_loss: 13.7623\n",
            "Epoch 57/100\n",
            "53/53 [==============================] - 24s 446ms/step - loss: 12.1468 - val_loss: 13.7685\n",
            "Epoch 58/100\n",
            "53/53 [==============================] - 24s 447ms/step - loss: 12.1451 - val_loss: 13.7688\n",
            "Epoch 59/100\n",
            "53/53 [==============================] - 24s 445ms/step - loss: 12.1447 - val_loss: 13.7708\n",
            "Epoch 60/100\n",
            "53/53 [==============================] - 24s 444ms/step - loss: 12.1441 - val_loss: 13.7736\n",
            "Epoch 61/100\n",
            "53/53 [==============================] - 24s 447ms/step - loss: 12.1430 - val_loss: 13.7747\n",
            "Epoch 62/100\n",
            "53/53 [==============================] - 24s 448ms/step - loss: 12.1449 - val_loss: 13.7737\n",
            "Epoch 63/100\n",
            "53/53 [==============================] - 24s 446ms/step - loss: 12.1416 - val_loss: 13.7741\n",
            "Epoch 64/100\n",
            "53/53 [==============================] - 24s 460ms/step - loss: 12.1412 - val_loss: 13.7782\n",
            "Epoch 65/100\n",
            "53/53 [==============================] - 24s 450ms/step - loss: 12.1399 - val_loss: 13.7733\n",
            "Epoch 66/100\n",
            "53/53 [==============================] - 24s 446ms/step - loss: 12.1392 - val_loss: 13.7774\n",
            "Epoch 67/100\n",
            "53/53 [==============================] - 24s 446ms/step - loss: 12.1377 - val_loss: 13.7796\n",
            "Epoch 68/100\n",
            "53/53 [==============================] - 23s 443ms/step - loss: 12.1376 - val_loss: 13.7853\n",
            "Epoch 69/100\n",
            "53/53 [==============================] - 24s 446ms/step - loss: 12.1383 - val_loss: 13.7837\n",
            "Epoch 70/100\n",
            "53/53 [==============================] - 24s 445ms/step - loss: 12.1375 - val_loss: 13.7877\n",
            "Epoch 71/100\n",
            "53/53 [==============================] - 23s 442ms/step - loss: 12.1362 - val_loss: 13.7915\n",
            "Epoch 72/100\n",
            "53/53 [==============================] - 24s 446ms/step - loss: 12.1361 - val_loss: 13.7958\n",
            "Epoch 73/100\n",
            "53/53 [==============================] - 24s 447ms/step - loss: 12.1357 - val_loss: 13.7988\n",
            "Epoch 74/100\n",
            "53/53 [==============================] - 24s 447ms/step - loss: 12.1340 - val_loss: 13.7972\n",
            "Epoch 75/100\n",
            "53/53 [==============================] - 24s 446ms/step - loss: 12.1332 - val_loss: 13.7949\n",
            "Epoch 76/100\n",
            "53/53 [==============================] - 28s 535ms/step - loss: 12.1345 - val_loss: 13.7975\n",
            "Epoch 77/100\n",
            "53/53 [==============================] - 24s 450ms/step - loss: 12.1324 - val_loss: 13.8004\n",
            "Epoch 78/100\n",
            "53/53 [==============================] - 24s 451ms/step - loss: 12.1307 - val_loss: 13.8027\n",
            "Epoch 79/100\n",
            "53/53 [==============================] - 24s 446ms/step - loss: 12.1319 - val_loss: 13.8035\n",
            "Epoch 80/100\n",
            "53/53 [==============================] - 24s 447ms/step - loss: 12.1313 - val_loss: 13.8059\n",
            "Epoch 81/100\n",
            "53/53 [==============================] - 24s 443ms/step - loss: 12.1300 - val_loss: 13.8104\n",
            "Epoch 82/100\n",
            "53/53 [==============================] - 24s 446ms/step - loss: 12.1296 - val_loss: 13.8143\n",
            "Epoch 83/100\n",
            "53/53 [==============================] - 24s 448ms/step - loss: 12.1293 - val_loss: 13.8196\n",
            "Epoch 84/100\n",
            "53/53 [==============================] - 24s 445ms/step - loss: 12.1243 - val_loss: 13.8148\n",
            "Epoch 85/100\n",
            "53/53 [==============================] - 24s 446ms/step - loss: 12.1274 - val_loss: 13.8176\n",
            "Epoch 86/100\n",
            "53/53 [==============================] - 24s 446ms/step - loss: 12.1250 - val_loss: 13.8199\n",
            "Epoch 87/100\n",
            "53/53 [==============================] - 24s 447ms/step - loss: 12.1247 - val_loss: 13.8273\n",
            "Epoch 88/100\n",
            "53/53 [==============================] - 24s 444ms/step - loss: 12.1229 - val_loss: 13.8281\n",
            "Epoch 89/100\n",
            "53/53 [==============================] - 24s 444ms/step - loss: 12.1229 - val_loss: 13.8320\n",
            "Epoch 90/100\n",
            "53/53 [==============================] - 24s 456ms/step - loss: 12.1217 - val_loss: 13.8345\n",
            "Epoch 91/100\n",
            "53/53 [==============================] - 24s 444ms/step - loss: 12.1226 - val_loss: 13.8350\n",
            "Epoch 92/100\n",
            "53/53 [==============================] - 24s 445ms/step - loss: 12.1214 - val_loss: 13.8356\n",
            "Epoch 93/100\n",
            "53/53 [==============================] - 24s 446ms/step - loss: 12.1204 - val_loss: 13.8366\n",
            "Epoch 94/100\n",
            "53/53 [==============================] - 24s 446ms/step - loss: 12.1217 - val_loss: 13.8432\n",
            "Epoch 95/100\n",
            "53/53 [==============================] - 24s 445ms/step - loss: 12.1209 - val_loss: 13.8432\n",
            "Epoch 96/100\n",
            "53/53 [==============================] - 24s 445ms/step - loss: 12.1170 - val_loss: 13.8466\n",
            "Epoch 97/100\n",
            "53/53 [==============================] - 24s 444ms/step - loss: 12.1177 - val_loss: 13.8421\n",
            "Epoch 98/100\n",
            "53/53 [==============================] - 24s 445ms/step - loss: 12.1184 - val_loss: 13.8465\n",
            "Epoch 99/100\n",
            "53/53 [==============================] - 24s 444ms/step - loss: 12.1167 - val_loss: 13.8458\n",
            "Epoch 100/100\n",
            "53/53 [==============================] - 24s 444ms/step - loss: 12.1186 - val_loss: 13.8467\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}